# deep-learning-Notebook
[contains deepdream, deep learning research papers] </br>
Artificial neural networks are among the earliest machine learning algorithms, and most are inspired by a particular simplification of the biological neuron. Interestingly neurons in the human visual system are sparsely connected, with connections mainly involving neurons associated with spatially neighboring areas of the retina. The best-performing learning algorithms on some vision tasks such as digit classification are convolutional networks, in which filters are both spatially sparse and shared across all the different filter locations. This motivated two variants explored here. 

<h4>Neural network papernotes</h4> 
This was inspired by Denny Britz when I checked some deep learning papers, so I believe it’s a good habit I should develop as well for documenting the realted papers I have read or some papers I have already categoried in my reading list. Of course there are so many I probably need more time to digest and add here (to do list). 

<ul>
  <li>
    Quadratic Polynomials Learn Better Image Features ∗ by James Bergstra, Guillaume Desjardins, Pascal Lamblin, Yoshua Bengio <a href='http://www.iro.umontreal.ca/~lisa/publications2/index.php/attachments/single/205'>
[Content]</a>
  </li>
  <li>
    Deep Sparse Rectifier Neural Networks by Xavier Glorot, Antoine Bordes et Yoshua Bengio <a href='http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf '>[Content] </a> 
  </li>
  <li>
    Practical Recommendations for Gradient-Based Training of Deep Architectures by Yoshua Bengio <a href='https://arxiv.org/pdf/1206.5533v1.pdf'>[ArXiv]</a> 
  </li>
  <li>
    Efficient Backprop by Yann LeCun, Leon Bottou, Genevieve B. Orr et Klaus-Robert Muller <a href='http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf'> [Content] </a> 
  </li>
  <li>
    Stochastic Gradient Descent Tricks by L´eon Bottou <a href='https://cilvr.cs.nyu.edu/diglib/lsml/bottou-sgd-tricks-2012.pdf'>[Content]</a>
  </li> 
  <li>
    Structured Learning and Prediction in Computer Vision by Sebastian Nowozin1 and Christoph H. Lampert <a href='http://www.nowozin.net/sebastian/papers/nowozin2011structured-tutorial.pdf'>[Content] </a> 
  </li>
  <li>
    An Introduction to Conditional Random Fields by Charles Sutton <a href='https://arxiv.org/pdf/1011.4088v1.pdf'>[ArXiv]</a>
  </li>
  <li>
    Conditional Neural Fields by Jian Peng and Liefeng Bo <a href='https://research.cs.washington.edu/istc/lfb/paper/nips09b.pdf'>[Content]</a>
  </li>
  <li>
    Neural conditional random fields by Trinh-Minh-Tri Do and Thierry Artieres <a href='http://publications.idiap.ch/downloads/papers/2010/Do_AISTATS_2010.pdf'>[Content] </a> 
  </li>
  <li>
    Gradient-Based Learning Applied to Document Recognition by Yann LeCun, Léon Bottou, Yoshua Bengio and Patrick Haffner <a href='http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf'>[Content] </a> 
  </li>
  <li>
    Inductive Principles for Restricted Boltzmann Machine Learning by Benjamin M. Marlin, Kevin Swersky, Bo Chen and Nando de Freitas <a href='https://people.cs.umass.edu/~marlin/research/papers/aistats2010-paper.pdf'>[Content] </a> 
  </li>
  <li>
    Noise-contrastive estimation: A new estimation principle for unnormalized statistical models by Michael Gutmann and Aapo Hyv¨arinen <a href='http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf'>[Content] </a>
  </li>
  <li>
    Classification using Discriminative Restricted Boltzmann Machines by Hugo Larochelle and Yoshua Bengio <a href='http://www.dmi.usherb.ca/~larocheh/publications/icml-2008-discriminative-rbm.pdf'>[Content] </a>
  </li>
  <li>
    Auto-Association by Multilayer Perceptrons and Singular Value Decomposition by Hervé Bourlard and Yves Kamp <a href='http://publications.idiap.ch/downloads/reports/2000/rr00-16.pdf'>[Content] </a>
  </li>
  <li>
    Gradient-based learning of higher-order image features by Roland Memisevic <a href='http://www.cs.toronto.edu/~rfm/pubs/rae.pdf'>[Content]</a> 
  </li>
  <li>
    A Fast Learning Algorithm for Deep Belief Nets by Geoffrey E. Hinton and Simon Osindero <a href='http://www.cs.toronto.edu/~hinton/absps/ncfast.pdf'> [Content]</a> 
  </li>
  <li>
    Deep Boltzmann Machines by Ruslan Salakhutdinov and Geoffrey Hinton <a href='http://www.utstat.toronto.edu/~rsalakhu/papers/dbm.pdf'>[Content]</a>
  </li>
  <li>
    Reducing the dimensionality of data with neural networks by Geoffrey Hinton and Ruslan Salakahutdinov <a href='http://www.cs.toronto.edu/~hinton/science.pdf'> [Content] </a>
  </li>
  <li>
    Learning Deep Architectures for AI by Yoshua Bengio <a href='http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf'>[Content]</a>
  </li>
  <li>
    An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation by Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, Yoshua Bengio <a href='http://www.dmi.usherb.ca/~larocheh/publications/deep-nets-icml-07.pdf'>[Content]</a>
  </li>
  <li>
    Exploring Strategies for Training Deep Neural Networks by Hugo Larochelle, Yoshua Bengio, Jerome  Louradour, Pascal Lamblin<a href='http://www.dmi.usherb.ca/~larocheh/publications/jmlr-larochelle09a.pdf'>[Content]</a> 
  </li>
  <li>
    Regularization of Neural Networks using DropConnect by Li Wan, Matthew Zeiler, Sixin Zhang, Yann LeCun and Rob Fergus <a href='https://cs.nyu.edu/~wanli/dropc/dropc.pdf'>[Content]</a> 
  </li>
  <li>
    Maxout Networks by Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville and Yoshua Bengio <a href='https://arxiv.org/pdf/1302.4389v4.pdf'>[ArXiv]</a>
  </li>
  <li>
    Self-taught Learning: Transfer Learning from Unlabeled Data by Rajat Raina, Alexis Battle, Honglak Lee, Benjamin Packer and Andrew Y. Ng <a href='https://cs.stanford.edu/people/ang/papers/icml07-selftaughtlearning.pdf'>[Content]</a>  
  </li>
  <li>
    Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition by Koray Kavukcuoglu, Marc’Aurelio Ranzato and Yann LeCun <a href='https://arxiv.org/pdf/1010.3467.pdf'>[ArXiv]</a>
  </li>
  <li>
    Independent Component Analysis: Algorithms and Applications by Aapo Hyvärinen and Erkki Oja <a href='https://www.cs.helsinki.fi/u/ahyvarin/papers/NN00new.pdf'>[Content]</a>
  </li>
  <li>
    Large-Scale Feature Learning With Spike-and-Slab Sparse Coding by Ian J. Goodfellow, Aaron Courville, Yoshua Bengio <a href='https://arxiv.org/pdf/1206.6407.pdf'>[ArXiv]</a> 
  </li>
  <li>
    ImageNet Classification with Deep Convolutional Neural Networks by Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton <a href='http://www.cs.utoronto.ca/~ilya/pubs/2012/imgnet.pdf'>[Content]</a>
  </li>
  <li>
    What is the Best Multi-Stage Architecture for Object Recognition? by Kevin Jarrett, Koray Kavukcuoglu, Marc’Aurelio Ranzato and Yann LeCun <a href='https://cs.nyu.edu/~koray/publis/jarrett-iccv-09.pdf'>[Content]</a> 
  </li>
  <li>
    Incorporating Complex Cells into Neural Networks for Pattern Classification by James Bergstra <a href='http://www.eng.uwaterloo.ca/~jbergstr/files/pub/11_These.pdf'>[Content]</a>
  </li>
  <li>
    Scene Parsing with Multiscale Feature Learning, Purity Trees, and Optimal Covers by Clément Farabet, Camille Couprie, Laurent Najman, Yann LeCun <a href='http://yann.lecun.com/exdb/publis/pdf/farabet-icml-12.pdf'>[Content]</a>
  </li>
  <li>
    Hierarchical Probabilistic Neural Network Language Model by Frederic Morin and Yoshua Bengio <a href='http://www.iro.umontreal.ca/~lisa/pointeurs/hierarchical-nnlm-aistats05.pdf'>[Content]</a>
  </li>
  <li>
    Advances in Markov chain Monte Carlo methods by Iain Murray <a href='https://homepages.inf.ed.ac.uk/imurray2/pub/07thesis/murray_thesis_2007.pdf'>[Content]</a>
  </li>
  <li>
    Natural Language Processing with Python by Steven Bird, Ewan Klein, and Edward Loper <a href='http://www.nltk.org/book/'></a>
  </li>
  <li>
    Tracking progress in Natural Language Processing <a href='https://nlpprogress.com/'>[Content]</a> 
  </li>
  <li>
    Natural Image Statistics, a probabilistic approach to early computational vision by Aapo Hyvarinen, Jarmo Hurri and Patrik O. Hoyer <a href='http://www.naturalimagestatistics.net/nis_preprintFeb2009.pdf'>[Content]</a>
  </li>
  <li>
    Going deeper with convolutions by Christian Szegedy,Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan and Vincent Vanhoucke, Andrew Rabinovich <a href='https://arxiv.org/pdf/1409.4842.pdf'>[ArXiv] </a> 
  </li>
</ul>

 
